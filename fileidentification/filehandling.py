import csv
import json
import os
import sys
from pathlib import Path
from typing import Any

import pygfried
from rich.progress import Progress, SpinnerColumn, TextColumn
from typer import colors, secho

from fileidentification.definitions.constants import CSVFIELDS, FMT2EXT
from fileidentification.definitions.models import (
    BasicAnalytics,
    FilePaths,
    LogMsg,
    LogOutput,
    LogTables,
    Mode,
    Policies,
    PoliciesFile,
    PolicyParams,
    SfInfo,
    sfinfo2csv,
)
from fileidentification.tasks.console_output import (
    print_diagnostic,
    print_duplicates,
    print_fmts,
    print_msg,
    print_processing_errors,
    print_siegfried_errors,
)
from fileidentification.tasks.conversion import convert_file
from fileidentification.tasks.inspection import inspect_file
from fileidentification.tasks.os_tasks import move_tmp, set_filepaths
from fileidentification.tasks.policies import apply_policy


class FileHandler:
    """Main class. It can create, verify and apply policies, test the files on errors, convert and move them."""

    def __init__(self) -> None:
        self.mode: Mode = Mode()
        self.policies: dict[str, PolicyParams] = {}
        self.log_tables = LogTables()
        self.ba = BasicAnalytics()
        self.stack: list[SfInfo] = []
        self.fp: FilePaths = FilePaths()
        self.config: dict[str, Any] = {}

    def _load_sfinfos(self, root_folder: Path) -> None:
        """
        Add sfinfos to stack.
        Checks whether a log json at default location exists. if so, it adds the sfinfos to the stack from there,
        otherwhise it scans the root_folder with pygfried and adds its output as sfinfos to the stack
        """
        initial = True
        # if there is a log, try to read from there
        if self.fp.LOG_J.is_file():
            initial = False
            self.stack.extend([SfInfo(**metadata) for metadata in json.loads(self.fp.LOG_J.read_text())["files"]])

        # else scan the root_folder with pygfried
        if not self.stack:
            with Progress(
                SpinnerColumn(), TextColumn("[progress.description]{task.description}"), transient=True
            ) as prog:
                prog.add_task(description="analysing files with pygfried...", total=None)
                self.stack.extend(
                    [
                        SfInfo(**pygfried.identify(f"{f}", detailed=True)["files"][0])  # type: ignore[arg-type]
                        for f in root_folder.rglob("*")
                        if f.is_file()
                    ]
                )
                if root_folder.is_file():
                    self.stack.append(SfInfo(**pygfried.identify(f"{root_folder}", detailed=True)["files"][0]))  # type: ignore[arg-type]

        # append path values run basic analytics
        for sfinfo in self.stack:
            if not sfinfo.status.removed:
                sfinfo.set_processing_paths(root_folder, self.fp.TMP_DIR, initial=initial)
            if not (sfinfo.status.removed or sfinfo.dest):
                self.ba.append(sfinfo)

        print_siegfried_errors(ba=self.ba)
        print_duplicates(ba=self.ba, mode=self.mode)

    # policies stuff
    def _load_policies(self, policies_path: Path) -> Policies:
        """Load and validate an existing policies.json"""
        if not policies_path.is_file():
            secho(f"{policies_path} not found", fg=colors.RED)
            sys.exit(1)
        try:
            file: PoliciesFile = PoliciesFile(**json.loads(policies_path.read_text()))
        except ValueError as e:
            secho(e, fg=colors.RED)
            sys.exit(1)

        self.policies = file.policies
        return file.policies

    def _gen_policies(self, outpath: Path, blank: bool = False, extend: bool = False) -> None:
        """
        Generate a policies.json with the default values of the encountered fileformats
        :param blank if set to True, it generates a blank policies.json
        :param extend if true, it expands the loaded policies with filetypes found in root_folder that are not in the
        loaded policies and writes out an updated policies.json
        """

        jsonfile = PoliciesFile(name=outpath)
        jsonfile.comment = "autogenerated"

        # blank caveat
        if blank:
            jsonfile.comment += " blank policies"
            for puid in self.ba.puid_unique:
                jsonfile.policies.update(
                    {puid: PolicyParams(format_name=FMT2EXT[puid]["name"], remove_original=self.mode.REMOVEORIGINAL)}
                )
            # write out policies with name of the folder, return policies
            jsonfile.name.write_text(jsonfile.model_dump_json(indent=4, exclude_none=True))
            self.policies = jsonfile.policies
            return

        # default values
        default_path = self.config["policies"]["DEFAULTPOLICIES"]
        default_policies = self._load_policies(Path(default_path))
        jsonfile.comment += f" using default policies {default_path}"
        jsonfile.comment += " in strict mode" if self.mode.STRICT else ""
        jsonfile.comment += f" updating from {outpath}" if extend else ""
        self.ba.blank = []
        for puid in self.ba.puid_unique:
            # if it is run in extend mode, add the existing policy if there is any
            if extend and puid in self.policies:
                jsonfile.policies.update({puid: self.policies[puid]})
            # if there are no default values of this filetype and not run in strict mode
            if not self.mode.STRICT and puid not in default_policies:
                jsonfile.policies.update({puid: PolicyParams(format_name=FMT2EXT[puid]["name"])})
                self.ba.blank.append(puid)
            if puid in default_policies:
                jsonfile.policies.update({puid: default_policies[puid]})
            # set remove original
            if puid in jsonfile.policies and self.mode.REMOVEORIGINAL:
                jsonfile.policies[puid].remove_original = self.mode.REMOVEORIGINAL

        # write out the policies with name of the folder
        self.policies = jsonfile.policies
        jsonfile.name.write_text(jsonfile.model_dump_json(indent=4, exclude_none=True))

    def _manage_policies(self, policies_path: Path | None = None, blank: bool = False, extend: bool = False) -> None:
        """
        Set the policies according to the parameters passed. either default policies, external passed policies or
        blank.
        """
        # default policies found and no external policies are passed
        if not policies_path and self.fp.POLICIES_J.is_file():
            # set default location
            policies_path = self.fp.POLICIES_J
        # no default policies found or the blank option is given:
        # fallback: generate the policies with optional flag blank
        if not policies_path or blank:
            policies_path = self.fp.POLICIES_J
            print_msg("... generating policies", self.mode.QUIET)
            self._gen_policies(policies_path, blank=blank)
        # load the external passed policies with option -p or default location
        else:
            print_msg(f"... loading policies from {policies_path}", self.mode.QUIET)
            self._load_policies(policies_path)

        # expand a passed policies with the filetypes found in root_folder that are not yet in the policies
        if extend and policies_path:
            print_msg(f"... updating the filetypes in policies {self.fp.POLICIES_J}", self.mode.QUIET)
            self._gen_policies(self.fp.POLICIES_J, extend=extend)

        print_fmts(list(self.ba.puid_unique), self.ba, self.policies, self.mode)

    def _test_policies(self, puid: str | None = None) -> None:
        """
        Test a policies.json with the smallest files of the directory. if puid is passed, it only tests the puid
        of the policies.
        """

        puids = [puid] if puid else [puid for puid in self.ba.puid_unique if not self.policies[puid].accepted]

        if not puids:
            print_msg("no files found that should be converted with given policies", self.mode.QUIET)
        else:
            print_msg("\n --- testing policies with a sample from the directory ---", self.mode.QUIET)

            for puid in puids:  # noqa: PLR1704
                # we want the smallest file first for running the test
                self.ba.sort_puid_unique_by_size(puid)
                sample = self.ba.puid_unique[puid][0]
                secho(f"\n{puid}", fg=colors.YELLOW)
                t_sfinfo, cmd = convert_file(sample, self.policies)
                if t_sfinfo:
                    secho(f"{cmd}", fg=colors.GREEN, bold=True)
                    secho(f"You find the file with the log in {t_sfinfo.filename.parent}")

    def inspect(self) -> None:
        print_msg("\nprobing the files ...", self.mode.QUIET)
        with Progress(SpinnerColumn(), transient=True) as prog:
            prog.add_task(description="", total=None)
            for sfinfo in self.stack:
                if not (sfinfo.status.removed or sfinfo.dest):
                    inspect_file(sfinfo, self.policies, self.log_tables, self.mode.VERBOSE)

        print_diagnostic(log_tables=self.log_tables, mode=self.mode)

    def apply_policies(self) -> None:
        print_msg("\napplying policies ...", self.mode.QUIET)
        with Progress(SpinnerColumn(), transient=True) as prog:
            prog.add_task(description="")
            for sfinfo in self.stack:
                if not (sfinfo.status.removed or sfinfo.dest):
                    apply_policy(sfinfo, self.policies, self.log_tables, self.mode.STRICT)

    def convert(self) -> None:
        """Convert files whose metadata status pending is True"""

        pending: list[SfInfo] = [sfinfo for sfinfo in self.stack if sfinfo.status.pending]

        if not pending:
            print_msg("there was nothing to convert", self.mode.QUIET)
            return

        print_msg("\nconverting ...", self.mode.QUIET)
        with Progress(SpinnerColumn(), transient=True) as prog:
            prog.add_task(description="", total=None)
            for sfinfo in pending:
                conv_sfinfo, cmd = convert_file(sfinfo, self.policies)
                if conv_sfinfo:
                    msg = f"converted -> {sfinfo.tdir.stem}/{conv_sfinfo.filename.parent.name}/{conv_sfinfo.filename.name}"
                    sfinfo.processing_logs.append(LogMsg(name="filehandler", msg=msg))
                    conv_sfinfo.root_folder = sfinfo.root_folder
                    self.stack.append(conv_sfinfo)
                else:
                    lmsg = sfinfo.processing_logs.pop()
                    lmsg.msg += f". cmd={cmd} "
                    self.log_tables.errors.append((lmsg, sfinfo))

    def remove_tmp(self, root_folder: Path, to_csv: bool = False) -> None:
        # move converted files from the working dir to its destination
        with Progress(SpinnerColumn(), transient=True) as prog:
            prog.add_task(description="", total=None)
            write_logs = move_tmp(self.stack, self.policies, self.log_tables, self.mode.REMOVEORIGINAL)

        # remove empty folders in working dir
        if self.fp.TMP_DIR.is_dir():
            for path, _, _ in os.walk(self.fp.TMP_DIR, topdown=False):
                if len(os.listdir(path)) == 0:  # noqa: PTH208
                    Path(path).rmdir()
        if write_logs:
            print_msg(f"\nmoved the files from {self.fp.TMP_DIR.stem} to {root_folder.stem} ...", self.mode.QUIET)
            self.write_logs(to_csv=to_csv)

    def write_logs(self, to_csv: bool = False) -> None:
        logoutput = LogOutput(files=self.stack, errors=self.log_tables.dump_errors())
        self.fp.LOG_J.write_text(logoutput.model_dump_json(indent=4, exclude_none=True))

        print_processing_errors(log_tables=self.log_tables)

        if to_csv:
            with open(f"{self.fp.LOG_J}.csv", "w") as f:  # noqa: PTH123
                w = csv.DictWriter(f, CSVFIELDS)
                w.writeheader()
                [w.writerow(sfinfo2csv(el)) for el in self.stack]

        sys.exit(0)

    # default run, has a typer interface for the params in identify.py
    def run(
        self,
        root_folder: Path | str,
        inspect: bool = True,
        apply: bool = True,
        remove_tmp: bool = True,
        convert: bool = False,
        policies_path: Path | None = None,
        blank: bool = False,
        extend: bool = False,
        test_puid: str | None = None,
        test_policies: bool = False,
        remove_original: bool = False,
        mode_strict: bool = False,
        mode_verbose: bool = True,
        mode_quiet: bool = True,
        to_csv: bool = False,
    ) -> None:
        root_folder = Path(root_folder)
        # set dirs / paths
        set_filepaths(self.fp, self.config, root_folder)
        # set the mode
        self.mode.REMOVEORIGINAL = remove_original
        self.mode.VERBOSE = mode_verbose
        self.mode.STRICT = mode_strict
        self.mode.QUIET = mode_quiet
        # generate a list of SfInfo objects out of the target folder
        self._load_sfinfos(root_folder)
        # generate policies
        self._manage_policies(policies_path, blank, extend)
        # convert caveat
        if convert:
            self.convert()
        # remove tmp caveat
        if remove_tmp:
            self.remove_tmp(root_folder, to_csv)
        # probing the files
        if inspect:
            self.inspect()
        # policies testing
        if test_puid:
            self._test_policies(puid=test_puid)
        if test_policies:
            self._test_policies()
        # apply policies
        if apply:
            self.apply_policies()
            self.convert()
        # remove tmp files
        if remove_tmp:
            self.remove_tmp(root_folder, to_csv)
        # write logs (if not called within remove_tmp)
        self.write_logs(to_csv=to_csv)
